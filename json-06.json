[{"figure_id": "1912.04023v3-Figure3-1", "captions": ["Fig. 3: Baseline fine-grained shading models. On the left, a standard encoder-decoder architecture (Eq. (2)), in the middle, Baseline-a with Squeeze ? and ? Excitation blocks (Hu et al., 2018), on the right, Baseline-b with extra decoders. ed denotes direct shading, e?a is for shadow casts and e + a is for ambient light, su is for unified shading, and ? is for reflectance.", "Since we are the first to estimate fine-grained shading intrinsics, we extend two versions of a state-of-theart model to provide a fair comparison. The modifications can be applied to any regular encoder-decoder type CNN architecture that is designed for the standard intrinsic image decomposition task. To this end, we extend the ShapeNet model (Shi et al., 2017). The network is designed to further enforce correlations between intrinsic components. Since we increase the number of intrinsic components to predict, the model is well equipped. Figure 3 illustrates the modifications."], "captions_norm": ["fig. number-tk baseline fine-grained shading models. on the left, a standard encoder-decoder architecture (eq. (2)), in the middle, baseline-a with squeeze ? and ? excitation blocks (hu et al., 2018), on the right, baseline-b with extra decoders. ed denotes direct shading, e?a is for shadow casts and e + a is for ambient light, su is for unified shading, and ? is for reflectance.", "since we are the first to estimate fine-grained shading intrinsics, we extend two versions of a state-of-theart model to provide a fair comparison. the modifications can be applied to any regular encoder-decoder type cnn architecture that is designed for the standard intrinsic image decomposition task. to this end, we extend the shapenet model (shi et al., 2017). the network is designed to further enforce correlations between intrinsic components. since we increase the number of intrinsic components to predict, the model is well equipped. figure number-tk illustrates the modifications."], "ocr_result": {"figure_id": "1912.04023v3-Figure3-1", "ocr_result": [{"text": "ed", "bbox": "[[954, 100], [980, 100], [980, 124], [954, 124]]", "confidence": "1.0"}, {"text": "red", "bbox": "[[642, 148], [684, 148], [684, 172], [642, 172]]", "confidence": "0.61"}, {"text": "~ea", "bbox": "[[938, 162], [978, 162], [978, 188], [938, 188]]", "confidence": "0.27"}, {"text": "Su", "bbox": "[[263, 177], [285, 177], [285, 195], [263, 195]]", "confidence": "1.0"}, {"text": "~ea", "bbox": "[[938, 224], [978, 224], [978, 250], [938, 250]]", "confidence": "0.47"}, {"text": "~ea", "bbox": "[[641.2572186472918, 184.14304661822948], [681.8782457042406, 190.31283693747292], [677.7427813527082, 212.85695338177052], [637.1217542957594, 206.68716306252708]]", "confidence": "0.86"}, {"text": "Jea", "bbox": "[[642.3859944401545, 217.0965827644789], [685.992796014412, 232.83040033919897], [678.6140055598455, 251.9034172355211], [635.007203985588, 237.16959966080103]]", "confidence": "0.11"}]}, "aspect": 3.78}, {"figure_id": "2105.03245v2-Figure2-1", "captions": ["Figure 2. Overview of AdaFocus. It first takes a quick glance at each frame vt using a light-weighted global CNN fG. Then a recurrent policy network ? is built on top of fG to select the most important image region v?t in terms of recognition. A high-capacity local CNN fL is adopted to extract features from v?t. Finally, a recurrent classifier aggregates the features across frames to obtain the prediction pt.", "Overview. We first give an overview of AdaFocus (Figure 2). Consider the online video recognition scenario, where a stream of frames come in sequentially while a prediction may be retrieved after processing any number of frames. At each time step, AdaFocus first takes a quick glance at the full frame with a light-weighted CNN f G , obtaining cheap and coarse global features. Then the features are fed into a recurrent policy network \u03c0 to aggregate the information across frames and accordingly determine the location of an image patch to be focused on, under the goal of maximizing its contribution to video recognition. A highcapacity local CNN f L is then adopted to process the se-lected patch for more accurate but computationally expensive representations. Finally, a classifier f C integrates the features of all previous frames to produce a prediction. In the following, we describe these four components in details."], "captions_norm": ["figure number-tk overview of adafocus. it first takes a quick glance at each frame vt using a light-weighted global cnn fg. then a recurrent policy network ? is built on top of fg to select the most important image region v?t in terms of recognition. a high-capacity local cnn fl is adopted to extract features from v?t. finally, a recurrent classifier aggregates the features across frames to obtain the prediction pt.", "overview. we first give an overview of adafocus (figure 2). consider the online video recognition scenario, where a stream of frames come in sequentially while a prediction may be retrieved after processing any number of frames. at each time step, adafocus first takes a quick glance at the full frame with a light-weighted cnn f g , obtaining cheap and coarse global features. then the features are fed into a recurrent policy network \u03c0 to aggregate the information across frames and accordingly determine the location of an image patch to be focused on, under the goal of maximizing its contribution to video recognition. a highcapacity local cnn f l is then adopted to process the se-lected patch for more accurate but computationally expensive representations. finally, a classifier f c integrates the features of all previous frames to produce a prediction. in the following, we describe these four components in details."], "ocr_result": {"figure_id": "2105.03245v2-Figure2-1", "ocr_result": [{"text": "Global CNN", "bbox": "[[154, 20], [312, 20], [312, 50], [154, 50]]", "confidence": "1.0"}, {"text": "fa", "bbox": "[[215, 51], [255, 51], [255, 93], [215, 93]]", "confidence": "0.37"}, {"text": "Forward", "bbox": "[[1554, 118], [1668, 118], [1668, 146], [1554, 146]]", "confidence": "1.0"}, {"text": "Policy", "bbox": "[[533, 173], [620, 173], [620, 214], [533, 214]]", "confidence": "0.64"}, {"text": "Network", "bbox": "[[520, 210], [632, 210], [632, 238], [520, 238]]", "confidence": "1.0"}, {"text": "T->", "bbox": "[[1601, 217], [1651, 217], [1651, 233], [1601, 233]]", "confidence": "0.05"}, {"text": "Identity", "bbox": "[[1558, 239], [1665, 239], [1665, 277], [1558, 277]]", "confidence": "1.0"}, {"text": "Prediction", "bbox": "[[1369, 269], [1501, 269], [1501, 299], [1369, 299]]", "confidence": "1.0"}, {"text": "fc", "bbox": "[[1289, 305], [1333, 305], [1333, 349], [1289, 349]]", "confidence": "1.0"}, {"text": "pt", "bbox": "[[1416, 312], [1460, 312], [1460, 344], [1416, 344]]", "confidence": "0.52"}, {"text": "Classifier", "bbox": "[[1250, 372], [1372, 372], [1372, 402], [1250, 402]]", "confidence": "0.92"}, {"text": "Updating", "bbox": "[[1552, 364], [1675, 364], [1675, 405], [1552, 405]]", "confidence": "1.0"}, {"text": "Image Patch @+", "bbox": "[[583, 391], [787, 391], [787, 430], [583, 430]]", "confidence": "0.5"}, {"text": "Local CNN f_", "bbox": "[[885, 387], [1069, 387], [1069, 429], [885, 429]]", "confidence": "0.94"}, {"text": "Hidden", "bbox": "[[1564, 400], [1660, 400], [1660, 430], [1564, 430]]", "confidence": "1.0"}, {"text": "State", "bbox": "[[1578, 434], [1648, 434], [1648, 462], [1578, 462]]", "confidence": "1.0"}, {"text": "Video Frame Ut", "bbox": "[[15, 449], [219, 449], [219, 485], [15, 485]]", "confidence": "0.76"}, {"text": "Recurrent", "bbox": "[[1548, 593], [1678, 593], [1678, 621], [1548, 621]]", "confidence": "0.98"}, {"text": "Networks", "bbox": "[[1551, 625], [1674, 625], [1674, 656], [1551, 656]]", "confidence": "1.0"}, {"text": "fc", "bbox": "[[1291, 697], [1333, 697], [1333, 741], [1291, 741]]", "confidence": "1.0"}, {"text": "Pt+1", "bbox": "[[1416, 702], [1492, 702], [1492, 734], [1416, 734]]", "confidence": "0.49"}, {"text": "Concatenation", "bbox": "[[1522, 768], [1704, 768], [1704, 800], [1522, 800]]", "confidence": "1.0"}, {"text": "Image Patch", "bbox": "[[574, 785], [735, 785], [735, 823], [574, 823]]", "confidence": "0.86"}, {"text": "Video Frame Ut+l", "bbox": "[[0, 839], [234, 839], [234, 877], [0, 877]]", "confidence": "0.89"}, {"text": "Crop", "bbox": "[[451.0513167019495, 309.1539501058485], [512.901838873174, 318.2388619349052], [507.9486832980505, 344.8460498941515], [446.098161126826, 335.7611380650948]]", "confidence": "0.75"}, {"text": "Crop", "bbox": "[[451.0220279500418, 701.1638810551212], [511.86943199038376, 708.1245800707304], [507.9779720499582, 735.8361189448788], [447.13056800961624, 727.8754199292696]]", "confidence": "1.0"}, {"text": "Ut+1", "bbox": "[[736.1322781687254, 778.0952517592642], [803.9223227027637, 797.2155354594472], [792.8677218312746, 830.9047482407358], [726.0776772972363, 812.7844645405528]]", "confidence": "0.66"}]}, "aspect": 1.9}, {"figure_id": "2107.11732v3-Figure3-1", "captions": ["Figure 3: Flowchart for Federated IPW-MLE", "for ATT weighting, where Our unrestricted federated IPW-MLE is built on our restricted federated MLE, and is similar to our unrestricted federated MLE. Parameters in propensity and/or outcome models are partitioned into shared and dataset-specific parameters. Our unrestricted federated IPW-MLE combines shared parameters across data sets to improve their estimation precision, and leaves dataset-specific parameters as they are in federation. We specify the parameters in the models for the combined data, whose dimensionality is higher than that for a specific data set. Next we properly zero-pad the estimated parameters and relevant matrices for every data set to match the dimensions of those for the combined data. The unrestricted estimator then follows the same federated procedure as the restricted estimator, but uses zero-padded parameters and matrices, as shown in Figure 3."], "captions_norm": ["figure number-tk flowchart for federated ipw-mle", "for att weighting, where our unrestricted federated ipw-mle is built on our restricted federated mle, and is similar to our unrestricted federated mle. parameters in propensity and/or outcome models are partitioned into shared and dataset-specific parameters. our unrestricted federated ipw-mle combines shared parameters across data sets to improve their estimation precision, and leaves dataset-specific parameters as they are in federation. we specify the parameters in the models for the combined data, whose dimensionality is higher than that for a specific data set. next we properly zero-pad the estimated parameters and relevant matrices for every data set to match the dimensions of those for the combined data. the unrestricted estimator then follows the same federated procedure as the restricted estimator, but uses zero-padded parameters and matrices, as shown in figure number-tk"], "ocr_result": {"figure_id": "2107.11732v3-Figure3-1", "ocr_result": [{"text": "YES", "bbox": "[[625, 3], [663, 3], [663, 21], [625, 21]]", "confidence": "1.0"}, {"text": "Coefficients:", "bbox": "[[846, 70], [962, 70], [962, 96], [846, 96]]", "confidence": "0.95"}, {"text": "Coefficients:", "bbox": "[[1628, 75], [1744, 75], [1744, 99], [1628, 99]]", "confidence": "1.0"}, {"text": "Propensity", "bbox": "[[60, 85], [161, 85], [161, 111], [60, 111]]", "confidence": "1.0"}, {"text": "Estimate individual", "bbox": "[[304, 84], [478, 84], [478, 108], [304, 108]]", "confidence": "0.85"}, {"text": "Propensity", "bbox": "[[574, 88], [674, 88], [674, 114], [574, 114]]", "confidence": "1.0"}, {"text": "Estimate individual", "bbox": "[[1074, 84], [1248, 84], [1248, 108], [1074, 108]]", "confidence": "0.77"}, {"text": "Outcome", "bbox": "[[1361, 83], [1450, 83], [1450, 109], [1361, 109]]", "confidence": "1.0"}, {"text": "model is known and", "bbox": "[[19, 111], [201, 111], [201, 131], [19, 131]]", "confidence": "0.64"}, {"text": "NO-{", "bbox": "[[235, 111], [271, 111], [271, 131], [235, 131]]", "confidence": "0.18"}, {"text": "propensity models", "bbox": "[[306, 108], [476, 108], [476, 134], [306, 134]]", "confidence": "0.98"}, {"text": "model is", "bbox": "[[584, 112], [666, 112], [666, 136], [584, 136]]", "confidence": "0.99"}, {"text": "YES", "bbox": "[[729, 109], [769, 109], [769, 129], [729, 129]]", "confidence": "0.96"}, {"text": "Hessian weighting;", "bbox": "[[816, 96], [990, 96], [990, 122], [816, 122]]", "confidence": "0.88"}, {"text": "outcome models", "bbox": "[[1084, 108], [1238, 108], [1238, 132], [1084, 132]]", "confidence": "1.0"}, {"text": "model is", "bbox": "[[1366, 108], [1448, 108], [1448, 132], [1366, 132]]", "confidence": "0.93"}, {"text": "YES", "bbox": "[[1511, 109], [1551, 109], [1551, 129], [1511, 129]]", "confidence": "1.0"}, {"text": "Hessian weighting;", "bbox": "[[1598, 98], [1772, 98], [1772, 128], [1598, 128]]", "confidence": "0.57"}, {"text": "used?", "bbox": "[[81, 135], [141, 135], [141, 155], [81, 155]]", "confidence": "1.0"}, {"text": "from MLE", "bbox": "[[342, 132], [436, 132], [436, 156], [342, 156]]", "confidence": "0.89"}, {"text": "stable?", "bbox": "[[588, 136], [660, 136], [660, 160], [588, 160]]", "confidence": "0.97"}, {"text": "Variance: Sample", "bbox": "[[822, 120], [986, 120], [986, 146], [822, 146]]", "confidence": "0.77"}, {"text": "from IPW-MLE", "bbox": "[[1090, 132], [1228, 132], [1228, 156], [1090, 156]]", "confidence": "1.0"}, {"text": "stable?", "bbox": "[[1370, 132], [1442, 132], [1442, 156], [1370, 156]]", "confidence": "1.0"}, {"text": "Variance: Sample", "bbox": "[[1604, 126], [1768, 126], [1768, 150], [1604, 150]]", "confidence": "0.56"}, {"text": "size weighting", "bbox": "[[836, 144], [970, 144], [970, 170], [836, 170]]", "confidence": "1.0"}, {"text": "size weighting", "bbox": "[[1618, 150], [1752, 150], [1752, 176], [1618, 176]]", "confidence": "1.0"}, {"text": "NO", "bbox": "[[609, 219], [639, 219], [639, 237], [609, 237]]", "confidence": "0.98"}, {"text": "Split shared and", "bbox": "[[692, 224], [842, 224], [842, 248], [692, 248]]", "confidence": "1.0"}, {"text": "NO", "bbox": "[[1391, 219], [1421, 219], [1421, 239], [1391, 239]]", "confidence": "0.98"}, {"text": "Split shared and", "bbox": "[[1472, 224], [1624, 224], [1624, 248], [1472, 248]]", "confidence": "0.95"}, {"text": "dataset-specific", "bbox": "[[694, 248], [840, 248], [840, 274], [694, 274]]", "confidence": "1.0"}, {"text": "dataset-specific", "bbox": "[[1476, 248], [1622, 248], [1622, 274], [1476, 274]]", "confidence": "0.95"}, {"text": "parameters; generate", "bbox": "[[668, 271], [866, 271], [866, 298], [668, 298]]", "confidence": "0.57"}, {"text": "parameters; generate", "bbox": "[[1448, 272], [1648, 272], [1648, 298], [1448, 298]]", "confidence": "0.71"}, {"text": "zero padding", "bbox": "[[706, 294], [828, 294], [828, 322], [706, 322]]", "confidence": "0.89"}, {"text": "zero padding", "bbox": "[[1486, 294], [1610, 294], [1610, 322], [1486, 322]]", "confidence": "0.76"}, {"text": "parameter vector and", "bbox": "[[668, 320], [866, 320], [866, 346], [668, 346]]", "confidence": "0.7"}, {"text": "parameter vector and", "bbox": "[[1449, 319], [1646, 319], [1646, 346], [1449, 346]]", "confidence": "1.0"}, {"text": "Hessian matrix", "bbox": "[[698, 344], [836, 344], [836, 368], [698, 368]]", "confidence": "1.0"}, {"text": "Hessian matrix", "bbox": "[[1478, 344], [1618, 344], [1618, 368], [1478, 368]]", "confidence": "1.0"}]}, "aspect": 4.8}, {"figure_id": "2002.02175v1-Figure1-1", "captions": ["Fig. 1. The overview of an autonomous driving model [37]", "Figure 1 shows the overview of an autonomous driving model. Given input data from sensors (e.g., LiDARs and cameras), a deep neural network predicts the control of the vehicle such as the steering angle and speed. CNN is the mainstream neural network architecture for autonomous driving, since it has excellent performance, requiring less neurons and consuming less resources. In autonomous vehicles, such driving model is usually included inside a perception domain controller, which can be updated remotely through the vehicle's gateway [21], [31]. Some companies have published their research on autonomous driving. For example, comma.ai presents a CNN based driving model to predict the steering angle based on driving video [34]. Nvidia builds a CNN model called DAVE-2 [2]. They demonstrate that DAVE-2 can automatically drive a vehicle without human intervention 90% of the time in a simulation test while performing autonomous steering 98% of time in an on-road test. "], "captions_norm": ["fig. number-tk the overview of an autonomous driving model citation-tk", "figure number-tk shows the overview of an autonomous driving model. given input data from sensors (e.g., lidars and cameras), a deep neural network predicts the control of the vehicle such as the steering angle and speed. cnn is the mainstream neural network architecture for autonomous driving, since it has excellent performance, requiring less neurons and consuming less resources. in autonomous vehicles, such driving model is usually included inside a perception domain controller, which can be updated remotely through the vehicle's gateway citation-tk. they demonstrate that davenumber-tk can automatically drive a vehicle without human intervention 90% of the time in a simulation test while performing autonomous steering 98% of time in an on-road test."], "ocr_result": {"figure_id": "2002.02175v1-Figure1-1", "ocr_result": [{"text": "Camera input", "bbox": "[[62, 30], [216, 30], [216, 62], [62, 62]]", "confidence": "0.97"}, {"text": "Lidar input", "bbox": "[[258, 29], [381, 29], [381, 62], [258, 62]]", "confidence": "1.0"}, {"text": "IR sensor input", "bbox": "[[423, 28], [599, 28], [599, 64], [423, 64]]", "confidence": "0.99"}, {"text": "In", "bbox": "[[648, 140], [676, 140], [676, 166], [648, 166]]", "confidence": "1.0"}, {"text": "05w", "bbox": "[[812, 172], [902, 172], [902, 222], [812, 222]]", "confidence": "0.22"}, {"text": "04", "bbox": "[[230, 256], [272, 256], [272, 284], [230, 284]]", "confidence": "0.68"}, {"text": "Activation", "bbox": "[[618, 278], [738, 278], [738, 310], [618, 310]]", "confidence": "1.0"}, {"text": "8666", "bbox": "[[240, 287], [410, 287], [410, 339], [240, 339]]", "confidence": "0.25"}, {"text": "function", "bbox": "[[628, 314], [728, 314], [728, 344], [628, 344]]", "confidence": "0.93"}, {"text": "0\"Y1;)", "bbox": "[[807, 295], [871, 295], [871, 335], [807, 335]]", "confidence": "0.18"}, {"text": "0*2)", "bbox": "[[216, 336], [260, 336], [260, 366], [216, 366]]", "confidence": "0.18"}, {"text": "Out", "bbox": "[[768, 402], [808, 402], [808, 426], [768, 426]]", "confidence": "0.35"}, {"text": "Steering angle", "bbox": "[[55, 439], [233, 439], [233, 479], [55, 479]]", "confidence": "1.0"}, {"text": "Brake", "bbox": "[[282, 446], [356, 446], [356, 474], [282, 474]]", "confidence": "0.75"}, {"text": "Accelerale", "bbox": "[[402, 446], [530, 446], [530, 476], [402, 476]]", "confidence": "0.89"}, {"text": "Individual neuron", "bbox": "[[666, 470], [868, 470], [868, 502], [666, 502]]", "confidence": "0.65"}, {"text": "684", "bbox": "[[410, 478], [510, 478], [510, 554], [410, 554]]", "confidence": "0.18"}]}, "aspect": 1.62}, {"figure_id": "2201.01232v2-Figure2-1", "captions": ["Figure 2. Overview of study design: COVID-19 detection and progression were assessed from audio data. Voice, cough, and breathing sound recordings were collected from each participant over a period, together with self-reported COVID-19 test results. During model development, audio recordings were chunked into segments consisting of 5 samples covering a few days and processed using sequential modeling techniques (GRUs) for COVID-19 monitoring. Two subtasks were evaluated: (1) COVID-19 detection (positive vs negative) and (2) disease progression monitoring. GRU: gated recurrent unit.", "We investigated whether longitudinally modeling audio biomarkers (cough, breath, and voice) can benefit COVID-19 detection and whether it could be used to monitor disease progression accurately and in a timely manner (Figure 2). The audio sequences were modeled by recurrent neural networks with gated recurrent units (GRUs) to consider audio dynamics, which reflect disease progression. The investigation was divided into 2 subtasks: one concerning COVID-19 detection by predicting audio biomarkers as positive and negative and the other concerning disease progression trajectory monitoring, examining the predicted probability of being positive over time. For instance, a decrease in the probability of being positive over time indicates a recovery trend, while an increase indicates a worsening trend. The first subtask aimed to assess whether modeling past audio biomarkers in the input space benefits COVID-19 detection in general, while the second subtask focused on longitudinal analysis of disease progression in the output space. "], "captions_norm": ["figure number-tk overview of study design: covidnumber-tk detection and progression were assessed from audio data. voice, cough, and breathing sound recordings were collected from each participant over a period, together with self-reported covidnumber-tk test results. during model development, audio recordings were chunked into segments consisting of number-tk samples covering a few days and processed using sequential modeling techniques (grus) for covidnumber-tk monitoring. two subtasks were evaluated: (1) covidnumber-tk detection (positive vs negative) and (2) disease progression monitoring. gru: gated recurrent unit.", "we investigated whether longitudinally modeling audio biomarkers (cough, breath, and voice) can benefit covidnumber-tk detection and whether it could be used to monitor disease progression accurately and in a timely manner (figure 2). the audio sequences were modeled by recurrent neural networks with gated recurrent units (grus) to consider audio dynamics, which reflect disease progression. the investigation was divided into number-tk subtasks: one concerning covidnumber-tk detection by predicting audio biomarkers as positive and negative and the other concerning disease progression trajectory monitoring, examining the predicted probability of being positive over time. for instance, a decrease in the probability of being positive over time indicates a recovery trend, while an increase indicates a worsening trend. the first subtask aimed to assess whether modeling past audio biomarkers in the input space benefits covidnumber-tk detection in general, while the second subtask focused on longitudinal analysis of disease progression in the output space."], "ocr_result": {"figure_id": "2201.01232v2-Figure2-1", "ocr_result": [{"text": "MMM", "bbox": "[[493, 96], [556, 96], [556, 135], [493, 135]]", "confidence": "0.08"}, {"text": "Audio Sequence", "bbox": "[[766, 84], [1052, 84], [1052, 136], [766, 136]]", "confidence": "0.69"}, {"text": "M", "bbox": "[[738, 196], [764, 196], [764, 228], [738, 228]]", "confidence": "0.55"}, {"text": "Mm", "bbox": "[[1014, 282], [1076, 282], [1076, 308], [1014, 308]]", "confidence": "0.42"}, {"text": "Maa", "bbox": "[[1111, 271], [1209, 271], [1209, 309], [1111, 309]]", "confidence": "0.15"}, {"text": "Mav", "bbox": "[[1269, 313], [1383, 313], [1383, 357], [1269, 357]]", "confidence": "0.03"}, {"text": "WAMAA", "bbox": "[[1380, 378], [1500, 378], [1500, 420], [1380, 420]]", "confidence": "0.07"}, {"text": "MAV", "bbox": "[[1658, 415], [1833, 415], [1833, 472], [1658, 472]]", "confidence": "0.07"}, {"text": "2", "bbox": "[[688, 502], [716, 502], [716, 536], [688, 536]]", "confidence": "0.43"}, {"text": "8", "bbox": "[[970, 502], [994, 502], [994, 532], [970, 532]]", "confidence": "0.89"}, {"text": "10", "bbox": "[[1107, 497], [1153, 497], [1153, 537], [1107, 537]]", "confidence": "1.0"}, {"text": "5", "bbox": "[[1260, 504], [1286, 504], [1286, 536], [1260, 536]]", "confidence": "1.0"}, {"text": "18", "bbox": "[[1367, 501], [1415, 501], [1415, 537], [1367, 537]]", "confidence": "1.0"}, {"text": "Days", "bbox": "[[1812, 494], [1907, 494], [1907, 542], [1812, 542]]", "confidence": "1.0"}, {"text": "Deep Neural Network (Gated Recurrent Units)", "bbox": "[[821, 794], [1616, 794], [1616, 847], [821, 847]]", "confidence": "0.97"}, {"text": "Negative", "bbox": "[[495, 941], [654, 941], [654, 994], [495, 994]]", "confidence": "0.92"}, {"text": "Positive", "bbox": "[[834, 947], [975, 947], [975, 989], [834, 989]]", "confidence": "0.73"}, {"text": "Negative", "bbox": "[[1517, 947], [1676, 947], [1676, 1000], [1517, 1000]]", "confidence": "0.66"}, {"text": "2", "bbox": "[[688, 1198], [714, 1198], [714, 1230], [688, 1230]]", "confidence": "1.0"}, {"text": "4", "bbox": "[[839, 1201], [859, 1201], [859, 1227], [839, 1227]]", "confidence": "1.0"}, {"text": "8", "bbox": "[[968, 1198], [994, 1198], [994, 1230], [968, 1230]]", "confidence": "0.99"}, {"text": "10", "bbox": "[[1107, 1195], [1153, 1195], [1153, 1233], [1107, 1233]]", "confidence": "1.0"}, {"text": "15", "bbox": "[[1239, 1197], [1287, 1197], [1287, 1233], [1239, 1233]]", "confidence": "1.0"}, {"text": "18", "bbox": "[[1371, 1197], [1415, 1197], [1415, 1233], [1371, 1233]]", "confidence": "1.0"}, {"text": "N", "bbox": "[[1752, 1198], [1780, 1198], [1780, 1230], [1752, 1230]]", "confidence": "1.0"}, {"text": "Days", "bbox": "[[1813, 1190], [1907, 1190], [1907, 1240], [1813, 1240]]", "confidence": "0.82"}, {"text": "COVID-19 Detection", "bbox": "[[502, 1259], [866, 1259], [866, 1308], [502, 1308]]", "confidence": "0.79"}, {"text": "Disease Trajectory Prediction", "bbox": "[[1052, 1260], [1572, 1260], [1572, 1312], [1052, 1312]]", "confidence": "1.0"}, {"text": "L", "bbox": "[[957, 1368], [997, 1368], [997, 1485], [957, 1485]]", "confidence": "0.81"}, {"text": "Onset?", "bbox": "[[1475, 1397], [1595, 1397], [1595, 1437], [1475, 1437]]", "confidence": "0.87"}, {"text": "8", "bbox": "[[960, 1482], [992, 1482], [992, 1518], [960, 1518]]", "confidence": "0.15"}, {"text": "Recovery?", "bbox": "[[1675, 1511], [1846, 1511], [1846, 1559], [1675, 1559]]", "confidence": "0.93"}, {"text": "Positive?", "bbox": "[[190, 1602], [339, 1602], [339, 1644], [190, 1644]]", "confidence": "0.91"}, {"text": "Negative?", "bbox": "[[463, 1599], [630, 1599], [630, 1648], [463, 1648]]", "confidence": "0.68"}, {"text": "[", "bbox": "[[952, 1518], [997, 1518], [997, 1681], [952, 1681]]", "confidence": "0.83"}, {"text": "Week 1", "bbox": "[[1057, 1629], [1171, 1629], [1171, 1665], [1057, 1665]]", "confidence": "0.89"}, {"text": "Week 2", "bbox": "[[1261, 1629], [1375, 1629], [1375, 1665], [1261, 1665]]", "confidence": "0.95"}, {"text": "Week 3", "bbox": "[[1467, 1627], [1583, 1627], [1583, 1665], [1467, 1665]]", "confidence": "1.0"}, {"text": "Week 4", "bbox": "[[1673, 1627], [1787, 1627], [1787, 1663], [1673, 1663]]", "confidence": "0.69"}, {"text": "Days", "bbox": "[[1800, 1627], [1882, 1627], [1882, 1671], [1800, 1671]]", "confidence": "0.55"}]}, "aspect": 1.18}, {"figure_id": "2110.05064v3-Figure2-1", "captions": ["Figure 2: PESNet\u2019s architecture is split into two main components, the MetaGNN and the WFModel. Circles indicate parameter-free and rectangles parametrized functions, ??? denotes the vector concatenation, A? and A? denote the index sets of the spin-up and spin-down electrons, respectively. To avoid clutter, we left out residual connections.", "Together, these components form PESNet, whose architecture is shown in Figure 2. Since sampling and energy computations only need the WFModel, a single forward pass of the MetaGNN is sufficient for each geometry during evaluation. Furthermore, its end-to-end differentiability facilitates optimization, see Section 3.4, and we may benefit from better generalization thanks to our equivariant wave function (Elesedy & Zaidi, 2021;Kondor & Trivedi, 2018).", "Ethics and reproducibility. Advanced computational chemistry tools may have a positive impact in chemistry research, for instance in material discovery. However, they also pose the risk of misuse, e.g., for the development of chemical weapons. To the best of our knowledge, our work does not promote misuse any more than general computational chemistry research. To reduce the likelihood of such misuse, we publish our source code under the Hippocratic license (Ehmke, 2019) 1 . To facilitate reproducibility, the source code includes simple scripts to reproduce all experiments from Section 4. Furthermore, we provide a detailed schematic of the computational graph in Figure 2 and additional details on the experimental setup including all hyperparameters in Appendix D."], "captions_norm": ["figure number-tk pesnet\u2019s architecture is split into two main components, the metagnn and the wfmodel. circles indicate parameter-free and rectangles parametrized functions, ??? denotes the vector concatenation, a? and a? denote the index sets of the spin-up and spin-down electrons, respectively. to avoid clutter, we left out residual connections.", "together, these components form pesnet, whose architecture is shown in figure number-tk since sampling and energy computations only need the wfmodel, a single forward pass of the metagnn is sufficient for each geometry during evaluation. furthermore, its end-to-end differentiability facilitates optimization, see section number-tk and we may benefit from better generalization thanks to our equivariant wave function (elesedy & zaidi, 2021;kondor & trivedi, 2018).", "ethics and reproducibility. advanced computational chemistry tools may have a positive impact in chemistry research, for instance in material discovery. however, they also pose the risk of misuse, e.g., for the development of chemical weapons. to the best of our knowledge, our work does not promote misuse any more than general computational chemistry research. to reduce the likelihood of such misuse, we publish our source code under the hippocratic license (ehmke, 2019) number-tk. to facilitate reproducibility, the source code includes simple scripts to reproduce all experiments from section number-tk furthermore, we provide a detailed schematic of the computational graph in figure number-tk and additional details on the experimental setup including all hyperparameters in appendix d."], "ocr_result": {"figure_id": "2110.05064v3-Figure2-1", "ocr_result": [{"text": "WFModel:", "bbox": "[[2, 0], [132, 0], [132, 28], [2, 28]]", "confidence": "1.0"}, {"text": "Wm", "bbox": "[[139, 3], [171, 3], [171, 19], [139, 19]]", "confidence": "1.0"}, {"text": "MetaGNN:", "bbox": "[[968, 0], [1102, 0], [1102, 26], [968, 26]]", "confidence": "1.0"}, {"text": "Zm", "bbox": "[[958, 56], [994, 56], [994, 82], [958, 82]]", "confidence": "0.99"}, {"text": "Embedding", "bbox": "[[1181, 54], [1284, 54], [1284, 84], [1181, 84]]", "confidence": "1.0"}, {"text": "MLP", "bbox": "[[1483, 57], [1527, 57], [1527, 77], [1483, 77]]", "confidence": "1.0"}, {"text": "Embedding", "bbox": "[[131, 112], [235, 112], [235, 144], [131, 144]]", "confidence": "0.83"}, {"text": "1!!!", "bbox": "[[290, 88], [458, 88], [458, 164], [290, 164]]", "confidence": "0.2"}, {"text": "Envelope", "bbox": "[[536, 106], [623, 106], [623, 137], [536, 137]]", "confidence": "1.0"}, {"text": "Wk det TY", "bbox": "[[734, 98], [828, 98], [828, 128], [734, 128]]", "confidence": "0.4"}, {"text": "Vb( r", "bbox": "[[868, 112], [924, 112], [924, 140], [868, 140]]", "confidence": "0.27"}, {"text": "Interaction", "bbox": "[[1185, 103], [1279, 103], [1279, 123], [1185, 123]]", "confidence": "1.0"}, {"text": "Interaction", "bbox": "[[1185, 143], [1279, 143], [1279, 163], [1185, 163]]", "confidence": "1.0"}, {"text": "MLP", "bbox": "[[1483, 155], [1527, 155], [1527, 175], [1483, 175]]", "confidence": "1.0"}, {"text": "Rn", "bbox": "[[942, 190], [980, 190], [980, 216], [942, 216]]", "confidence": "1.0"}, {"text": "RBF", "bbox": "[[1055, 187], [1099, 187], [1099, 207], [1055, 207]]", "confidence": "1.0"}, {"text": "Interaction", "bbox": "[[1185, 187], [1279, 187], [1279, 207], [1185, 207]]", "confidence": "0.98"}, {"text": "Embedding:", "bbox": "[[2, 239], [155, 239], [155, 279], [2, 279]]", "confidence": "1.0"}, {"text": "Update:", "bbox": "[[395, 243], [495, 243], [495, 279], [395, 279]]", "confidence": "1.0"}, {"text": "Interaction:", "bbox": "[[1076, 250], [1220, 250], [1220, 278], [1076, 278]]", "confidence": "1.0"}, {"text": "MLP:", "bbox": "[[1394, 252], [1470, 252], [1470, 278], [1394, 278]]", "confidence": "1.0"}, {"text": "LWF", "bbox": "[[966, 328], [1012, 328], [1012, 354], [966, 354]]", "confidence": "0.59"}, {"text": "o(Wx + b)", "bbox": "[[1438, 320], [1542, 320], [1542, 346], [1438, 346]]", "confidence": "0.42"}, {"text": "Envelope: h;", "bbox": "[[822, 339], [979, 339], [979, 384], [822, 384]]", "confidence": "1.0"}, {"text": "ill", "bbox": "[[1126, 344], [1156, 344], [1156, 370], [1126, 370]]", "confidence": "0.68"}, {"text": "emn", "bbox": "[[1163, 357], [1203, 357], [1203, 375], [1163, 375]]", "confidence": "1.0"}, {"text": "o(Wx + 6)", "bbox": "[[1438, 366], [1542, 366], [1542, 392], [1438, 392]]", "confidence": "0.6"}, {"text": "Wx", "bbox": "[[132, 412], [172, 412], [172, 436], [132, 436]]", "confidence": "0.84"}, {"text": "Wx + b", "bbox": "[[1454, 412], [1528, 412], [1528, 436], [1454, 436]]", "confidence": "0.46"}, {"text": "MLP", "bbox": "[[1273, 441], [1317, 441], [1317, 461], [1273, 461]]", "confidence": "1.0"}, {"text": "Wx + b", "bbox": "[[948, 470], [1024, 470], [1024, 494], [948, 494]]", "confidence": "0.42"}, {"text": "orbital", "bbox": "[[835, 487], [895, 487], [895, 503], [835, 503]]", "confidence": "0.98"}, {"text": "bembed", "bbox": "[[12, 506], [84, 506], [84, 534], [12, 534]]", "confidence": "1.0"}, {"text": "MLP", "bbox": "[[148, 506], [208, 506], [208, 532], [148, 532]]", "confidence": "1.0"}, {"text": "bdouble", "bbox": "[[335, 491], [413, 491], [413, 531], [335, 531]]", "confidence": "1.0"}, {"text": "(Wz + 6)", "bbox": "[[704, 498], [794, 498], [794, 522], [704, 522]]", "confidence": "0.41"}, {"text": "k,a", "bbox": "[[849, 513], [879, 513], [879, 531], [849, 531]]", "confidence": "0.96"}, {"text": "\"m", "bbox": "[[839, 529], [869, 529], [869, 547], [839, 547]]", "confidence": "0.49"}, {"text": "bsingle `", "bbox": "[[341, 543], [409, 543], [409, 583], [341, 583]]", "confidence": "0.7"}, {"text": "Wc + b", "bbox": "[[436, 550], [510, 550], [510, 574], [436, 574]]", "confidence": "0.61"}, {"text": "Wx", "bbox": "[[619, 551], [655, 551], [655, 571], [619, 571]]", "confidence": "0.84"}, {"text": "MLP", "bbox": "[[1271, 613], [1317, 613], [1317, 633], [1271, 633]]", "confidence": "1.0"}, {"text": "h}", "bbox": "[[164, 696], [188, 696], [188, 724], [164, 724]]", "confidence": "0.3"}, {"text": "ka", "bbox": "[[979, 705], [1001, 705], [1001, 721], [979, 721]]", "confidence": "0.91"}, {"text": "t+1", "bbox": "[[1281, 707], [1313, 707], [1313, 723], [1281, 723]]", "confidence": "0.24"}, {"text": "Wm", "bbox": "[[1616.1401328392153, 59.194278962352136], [1651.965140491394, 67.6282166637903], [1647.8598671607847, 83.80572103764786], [1611.034859508606, 75.3717833362097]]", "confidence": "0.96"}, {"text": "Rm", "bbox": "[[29.140132839215305, 108.19427896235214], [68.89736659610102, 113.36754446796633], [65.8598671607847, 132.80572103764786], [26.102633403898974, 127.63245553203367]]", "confidence": "0.99"}, {"text": "Rm", "bbox": "[[880.0513167019495, 185.15395010584845], [923.8821425485235, 196.16738104153768], [917.9486832980505, 220.84604989415155], [873.1178574514765, 210.83261895846232]]", "confidence": "1.0"}, {"text": "Rm", "bbox": "[[159.3053712883528, 241.12450247855259], [201.9144228545972, 249.42122099744736], [197.6946287116472, 271.8754975214474], [155.0855771454028, 263.57877900255266]]", "confidence": "1.0"}, {"text": "Rm", "bbox": "[[1044.0220279500418, 338.1638810551212], [1087.8770531241767, 348.1499615769478], [1081.9779720499582, 372.8361189448788], [1038.1229468758233, 362.8500384230522]]", "confidence": "1.0"}, {"text": "zm", "bbox": "[[57.02871413764274, 414.2516854477569], [86.84803366484378, 418.2352964145474], [83.97128586235726, 435.7483145522431], [54.15196633515622, 431.7647035854526]]", "confidence": "0.78"}, {"text": "Zmn", "bbox": "[[193.06850713433477, 416.23016355523606], [224.88485549096248, 421.33118030965846], [221.93149286566523, 438.76983644476394], [190.11514450903752, 433.66881969034154]]", "confidence": "0.16"}, {"text": "bk44", "bbox": "[[817.5430712064641, 465.3775281716354], [879.5724787771376, 458.45651273371743], [881.4569287935359, 488.6224718283646], [819.4275212228624, 494.54348726628257]]", "confidence": "0.37"}, {"text": "ht+1", "bbox": "[[519.5423042384429, 701.27961296691], [570.1763910955276, 685.935218704243], [578.4576957615571, 711.72038703309], [526.8236089044724, 727.064781295757]]", "confidence": "0.87"}]}, "aspect": 2.24},